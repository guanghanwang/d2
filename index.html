<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="d2 Project Page">
  <meta property="og:title" content="d2"/>
  <meta property="og:description" content="d2: Improved Techiniques for Training Reasoniong Diffusion Language Models"/>
  <meta property="og:url" content="guanghanwang.com/d2"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/9025770_gear_icon.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>d2 project page</title>
  <link rel="icon" type="image/x-icon" href="static/images/99025770_gear_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">d2: Improved Techinques for Training Reasonoing Diffusion Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://guanghanwang.com" target="_blank">Guanghan Wang<sup>1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://giladturok.github.io/" target="_blank">Gilad Turok<sup>1</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://yair-schiff.github.io/" target="_blank">Yair Schiff<sup>1</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://m-arriola.com/" target="_blank">Marianne Arriola<sup>1</sup></a>,</span>
                      <span class="author-block">
                        <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov<sup>1</sup></a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Cornell&nbsp;<img src="static/images/cornell.png" alt="Cornell" style="float:right;width:30px;height:30px;"></span>
                  </div>
                  

                  <div class="column has-text-centered">

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2509.21474" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/kuleshov-group/remdm" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                  <!-- YouTube link
                  <span class="link-block">
                    <a href="https://www.youtube.com/watch?v=juZA_vdjjv8" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>YouTube</span>
                  </a>
                  </span> -->

                  <!-- Colab link -->
                  <!-- <span class="link-block">
                    <a href="https://colab.research.google.com/drive/143sxeasXbmUIdROSXYQJLmSMIAhSZCPw?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://colab.research.google.com/img/colab_favicon_256px.png" 
                            alt="Google Colab" 
                            style="height: 1em; vertical-align: middle;">
                      </span>
                      <span>Colab</span>
                    </a>
                  </span> -->

                  <!-- <p style="text-align: center; margin-top: 12px; font-size: 18px;">
                    <strong>NeurIPS 2025</strong>
                  </p> -->

                  <p style="text-align: center; margin-top: 20px; font-size: 24px;">
                    <strong>TL;DR:</strong> <span>An efficient and effective algorithm to improve reasoning post-training for diffusion language models via reinforcement learning
                    </span>
                  </p>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Graphical Abstract -->
<section class="section" id="Graphical Abstract"></section>
<div class="container is-max-desktop">
  <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
    <img src="static/images/graphical_abstract.png" alt="Caduceus" style="width:2000px;height:280px;">
    <br>
      Our reinforcement learning (RL) post-training framework for diffusion language models, d2, consistently outperforms previous baselines, such as 
      d1 (<a href="https://arxiv.org/abs/2504.12216">
      Zhao et al., 2025</a>), and wd1 (<a href="https://arxiv.org/abs/2507.08838">
      Tang et al., 2025</a>) across four math and logical reasoning benchmarks. While previous methods, such as d1, relies on chain-of-thought supervised finetuning (SFT) to enhance 
      reasoning performance, d2 is able to outperform them without SFT, verifying the efficacy of our proposed RL algorithm.
  </h4>
  <br>
</div>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="content is-medium">
        <h2 class="title">Our contributions</h2>
        <ul>
          <li>We derive a novel GRPO-style RL policy gradient algorithm for masked DLMs, highlighting the importance 
            of accurate trajectory likelihood estimates for training reasoning diffusion language models.</li>
          <li>We introduce <strong>d2-AnyOrder</strong>, which enables unbiased one-pass likelihood estimates of sample trajectories 
            in DLMs that support <strong>any-order decoding</strong>. Empirically, d2-AnyOrder significantly outperforms widely-used 
            RL baselines such as diffu-GRPO (<a href="https://arxiv.org/abs/2504.12216"> Zhao et al., 2025</a>) and 
            DDPO (<a href="https://arxiv.org/abs/2305.13301"> Black et al., 2023</a>).</li>
          <li>For DLMs that do not naturally support any-order decoding, we propose <strong>d2-StepMerge</strong> to provide practical 
            likelihood approximation. When applied to LLaDA-8B-Instruct, d2-StepMerge achieves state-of-the-art reasoning 
            results on Sudoku, Countdown, GSM8K, and MATH500, without relying on supervised fine-tuning.</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Motivation -->
<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <h2 class="title">Motivation</h2>
    <div class="content is-medium">
      Masked diffusion language models (DLMs) have recently emerged as a competitive alternative to autoregressive (AR) models for language generation. 
      Yet, while reinforcement learning (RL) has become the de-facto approach for inducing reasoning in AR LLMs, post-training DLMs using RL
      remains an active research area due to the complexity of DLM policies.<br>  

      <br>In this project, we study the problem of RL for DLMs in a principled way. Concretely, we first dervie the GRPO objective of masked DLMs 
      starting from the policy gradient estimation. We then propose practical approaches for conducting RL on masked DLMs inspired by our theorectical
      derviation, which we find to work well empirically compared to previous RL baselines.
    </div>
  </div>
</section>
<!-- End Motivation-->


<!-- Background -->
<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <h2 class="title">Deriving the GRPO objective for masked DLMs</h2>
    <div class="content is-medium">
      <!-- Given a clean one-hot representation of a data token \(\mathbf{x} \in \mathbb{R}^{|V|} \), where \(|V|\) denotes the vocabulary size, masked diffusion models
      design a forward Markov chain process \(q\) to gradually add noise to \(\mathbf{x}\) and learn a parameterized model \(p_\theta\) to fit the corresponding
      reverse process. Specifically, the forward process \(q\) is handcrafted and the marginal of each latent variable \(\mathbf{z}_t\) takes the following 
      form.
      $$
      q(\mathbf{z}_t|\mathbf{x}) = \mathrm{Cat}(\mathbf{z}_t;\alpha_t\mathbf{x} + (1 - \alpha_t)\boldsymbol{\pi}) 
      $$
      \(\mathrm{Cat}\) denotes the categorical distribution. We follow the normalized time representation as in denoising diffusion models, i.e., \(t \in [0, 1]\),
      and \(\alpha_t\) is a monotonically decreasing scalar variable with \(\alpha_0 \approx 1\) and \(\alpha_1 \approx 0\). \( \boldsymbol{\pi} \) is the one-hot representation
      of a special \([MASK]\) token (represented as \(\mathbf{m}\)).  -->
      <!-- Given a clean one-hot representation of a data token \(\mathbf{x} \in \mathbb{R}^{|V|} \), where \(|V|\) denotes the vocabulary size, masked diffusion models
      design a forward Markov chain process \(q\) to gradually add noise to \(\mathbf{x}\) towards some limiting distribution \(\mathbf{m}\), which corresponds to a one-hot represesntation of a special \([MASK]\) token.
      Specifically, the forward process \(q\) is handcrafted and the marginal of each latent variable \(\mathbf{z}_t\) for \(t \in [0, 1]\), takes the following form.
      $$
      q(\mathbf{z}_t|\mathbf{x}) = \mathrm{Cat}(\mathbf{z}_t;\alpha_t\mathbf{x} + (1 - \alpha_t)\mathbf{m}), 
      $$
      where \(\mathrm{Cat}\) denotes the categorical distribution and \(\alpha_t\) is a monotonically decreasing scalar variable with \(\alpha_0 \approx 1\) and \(\alpha_1 \approx 0\).

      Intuitionally, for each clean token \(\mathbf{x}\), the forward process picks a time \(t\) and mask it, and the probability of 
      \(\mathbf{x}\) remaining unmasked at time \(t\) is \(\alpha_t\). By Bayes' rule, the ground-truth posterior takes the following form.
      $$
      q(\mathbf{z}_s|\mathbf{z}_t, \mathbf{x}) = \mathrm{Cat}(\mathbf{z}_s; \frac{\alpha_s - \alpha_t}{1 - \alpha_t}\mathbf{x} + \frac{1 - \alpha_s}{1 - \alpha_t}\mathbf{z}_t)
      $$
      \(s\) denotes the time step that directly precedes \(t\). To model this, we use a neural network \(\mathbf{x}_\theta(\mathbf{z}_t)\) to recover \(\mathbf{x}\)
      and set the parameterized posterior as \(p_\theta(\mathbf{z}_s|\mathbf{z}_t) = q(\mathbf{z}_s|\mathbf{z}_t, \mathbf{x}_\theta(\mathbf{z}_t))\). To train the model, we use the 
      technique of variational inference as in continuous diffusion models. To simplify the training objective, two constraints on \(\textbf{x}_\theta\) are applied.
      Particularly, when \(\mathbf{z}_t = \mathbf{m}\), \(\textbf{x}_\theta(\textbf{z}_t)^\top\textbf{m} = 0\) and when \(\mathbf{z}_t \neq \textbf{m}\), \(\textbf{x}_\theta(\textbf{z}_t) = \textbf{z}_t\).
      With the help of the two proposed constraints, the training objective takes the following form.
      $$
      \mathcal{L} = \mathbb{E}_{t \in \{\frac{1}{T}, \ldots, 1\}, \mathbf{z}_{0:T} \sim q(\mathbf{z}_{0:T}|\mathbf{x})} \bigg[\underbrace{-\log p_\theta(\mathbf{x}|\mathbf{z}_0)}_{\mathcal{L}_{reconstruct}} + \underbrace{T\frac{\alpha_t - \alpha_s}{1-\alpha_t}\log(\mathbf{x}_\theta(\mathbf{z}_t)^\top\mathbf{x}))}_{\mathcal{L}_{diffusion}} + \underbrace{D_{KL}(q(\mathbf{z}_T|\mathbf{x})\Vert p_\theta(\mathbf{z}_T))}_{\mathcal{L}_{prior}}\bigg]
      $$
      \(T\) is the predefined number of sampling steps and \(D_{KL}\) denotes the compuation of KL divergence. Following <a href="https://arxiv.org/abs/2406.07524">
        Sahoo et al. 2024</a>, we refer to this model as MDLM.<br>
      <br><strong>Failure-to-remask property</strong>: As mentioned above, when \(\mathbf{z}_t \neq \mathbf{m}\), \(\mathbf{x}_\theta(\mathbf{z}_t)=\mathbf{z}_t\). This deduces that 
      \(p_\theta(\mathbf{z}_s|\mathbf{z}_t) = \mathrm{Cat}(\mathbf{z}_s; \mathbf{z}_t)\). Intuitionally, in the sampling process, once a token is unmasked, it stays unchanged until the end of the generation process.
      We refer to this as the failure-to-remask property.  -->
      <!-- Given a clean one-hot representation of a data token \(\mathbf{x} \in \mathbb{R}^{|V|} \), where \(|V|\) denotes the vocabulary size, masked diffusion models
      design a forward Markov chain process \(q\) to gradually add noise to \(\mathbf{x}\) towards some limiting distribution \(\mathbf{m}\), which corresponds to a one-hot represesntation of a special \([MASK]\) token.
      Specifically, the forward process \(q\) is handcrafted and the marginal of each latent variable \(\mathbf{z}_t\) for \(t \in [0, 1]\), takes the following form.
      $$
      q(\mathbf{z}_t|\mathbf{x}) = \mathrm{Cat}(\mathbf{z}_t;\alpha_t\mathbf{x} + (1 - \alpha_t)\mathbf{m}), 
      $$
      where \(\mathrm{Cat}\) denotes the categorical distribution and \(\alpha_t\) is a monotonically decreasing scalar variable with \(\alpha_0 \approx 1\) and \(\alpha_1 \approx 0\).
      Intuitively, this process corresponds to each clean token \(\mathbf{x}\) 'picking a time' \(t\) and mask it, and the probability of \(\mathbf{x}\) remaining unmasked at time \(t\) is \(\alpha_t\). -->
      <!-- Letting \(s\) denote the time step directly preceding \(t\), by Bayes' rule, the ground-truth posterior takes the following form:
      $$
      q(\mathbf{z}_s|\mathbf{z}_t, \mathbf{x}) = \mathrm{Cat}(\mathbf{z}_s; \frac{\alpha_s - \alpha_t}{1 - \alpha_t}\mathbf{x} + \frac{1 - \alpha_s}{1 - \alpha_t}\mathbf{z}_t)
      $$
      
      The goal of diffusion modeling is to learn a parameterized model \(p_\theta\) to fit the corresponding reverse process.
      We parameterize this model with a neural network \(\mathbf{x}_\theta(\mathbf{z}_t)\) that aims to recover \(\mathbf{x}\) given some noised latent at timestep \(t\), and we set the parameterized posterior as \(p_\theta(\mathbf{z}_s|\mathbf{z}_t) = q(\mathbf{z}_s|\mathbf{z}_t, \mathbf{x}_\theta(\mathbf{z}_t))\).
      To train the model, we use the technique of variational inference as in continuous diffusion models and minimize the following objective: -->
      <!-- To simplify the training objective, two constraints on \(\textbf{x}_\theta\) are applied. -->
      <!-- Particularly, when \(\mathbf{z}_t = \mathbf{m}\), \(\textbf{x}_\theta(\textbf{z}_t)^\top\textbf{m} = 0\) and when \(\mathbf{z}_t \neq \textbf{m}\), \(\textbf{x}_\theta(\textbf{z}_t) = \textbf{z}_t\). -->
      <!-- With the help of the two proposed constraints, the training objective takes the following form. -->
      <!-- $$
      \mathcal{L} = \mathbb{E}_{t \in \{\frac{1}{T}, \ldots, 1\}, \mathbf{z}_{0:T} \sim q(\mathbf{z}_{0:T}|\mathbf{x})} \bigg[\underbrace{-\log p_\theta(\mathbf{x}|\mathbf{z}_0)}_{\mathcal{L}_{reconstruct}} + \underbrace{T\frac{\alpha_t - \alpha_s}{1-\alpha_t}\log(\mathbf{x}_\theta(\mathbf{z}_t)^\top\mathbf{x}))}_{\mathcal{L}_{diffusion}} + \underbrace{D_{KL}(q(\mathbf{z}_T|\mathbf{x})\Vert p_\theta(\mathbf{z}_T))}_{\mathcal{L}_{prior}}\bigg],
      $$
      where \(T\) is the predefined number of sampling steps and \(D_{KL}\) denotes the compuation of KL divergence. Of note, the probability distribution at time \(T\) is a delta distribution centered at the \([MASK]\) token, rendering  
      \(\mathcal{L}_{prior}\) as 0 and thus could be omitted. Following <a href="https://arxiv.org/abs/2406.07524"> Sahoo et al. 2024</a>, we refer to this model as MDLM.<br>      
      <br><strong>Failure to remask property</strong>: Following the form for the posterior above, when \(\mathbf{z}_t \neq \mathbf{m}\), the distribution is a discrete delta function concentrated on \(\mathbf{z}_t=\mathbf{x}\).
      We thus have \(p_\theta(\mathbf{z}_s|\mathbf{z}_t) = \mathrm{Cat}(\mathbf{z}_s; \mathbf{z}_t)\), which implies that in the sampling process, once a token is unmasked, it stays unchanged until the end of the generation process.
      We refer to this as the failure to remask property. -->
      We start our derivation by defining the policy gradient objective for a masked DLM policy \(\pi_\theta\) as:

      $$
      \nabla_\theta\mathcal{J}(\theta) = \nabla_\theta \mathbb{E}_{\mathbf{x}_0^{1:L}\sim\pi_\theta(\cdot\mid\mathbf{q})}\Big[r(\mathbf{x}_0^{1:L}, \mathbf{q})\Big].
      $$

      Here, \(\mathbf{q}\) denotes the prompt sequence, \(r\) represents the reward funciton, and \(\mathbf{x}_0^{1:L}\) denotes the
      \(L\) sample tokens generated by the model. Based on our derivation (see the full derivation chain in our paper), the GRPO objective for masked DLMs is given by:
      
      $$
      -\mathbb{E}_{\mathbf{x}_0^{1:L}\sim\pi_{\textnormal{old}}} \Big[\sum_{t=0}^{T-1}\frac{1}{L}\sum_{l=1}^L\mathbf{1}_{t,l}\textnormal{min}(\rho_t^lA^l, 
      \textnormal{clip}(\rho_t^l, 1-\epsilon, 1+\epsilon)A^l) 
      +\beta D_{\mathrm{KL}}\Big(\pi_\theta(\mathbf{x}_{0:T}^{1:L}\mid \mathbf{q})\Vert \pi_{\textnormal{ref}}(\mathbf{x}_{0:T}^{1:L}\mid \mathbf{q}))\Big)\Big].
      $$
      
      where \(A^l\) denotes the advantage values of the corresponding sample sequence, \(\rho_t^l=\frac{\pi_\theta(\mathbf{x}_t^l\mid\mathbf{x}_{t+1}^{1:L}, \mathbf{q})}{\pi_{\textnormal{old}}(\mathbf{x}_t^l \mid \mathbf{x}_{t+1}^{1:L}, \mathbf{q})}\) denotes the importance sampling weight, 
      and \(\mathbf{1}_{t,l}\) is the indicator of tokens decoded at time step \(t\), i.e., \(\mathbf{1}_{t,l}=\mathbf{1}_{\mathbf{x}_{t+1}^l=\mathbf{m}, \mathbf{x}_t^l\neq\mathbf{m}}\).<br>
      
      <br>To compute this objective, we need to estimate the <strong>trajectory likelihood</strong> of a sequence.
      In other words, for each token in a sample squence, we should compute its likelihood at its decoding time step. However, for standard masked DLMs such as LLaDA,
      computing the trajectory likelihood of a sample sequence decoded in \(T\) time steps takes \(T\) model forward passes, rendering it computationally prohibitive. 
      Based on this observation, we propose several pratical techinques to compute masked DLMs' trajectory likelihood in an <strong>efficient and effective</strong> way.

    </div>
  </div>
</section>
<!-- End Background-->


<!-- Sampler -->
<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <h2 class="title">d2-AnyOrder: an unbiased, one-model pass trajectory likelihood estimator</h2>
    <div class="content is-medium">
      <h3 class="subtitle" style="font-size:1.6rem;"><em>Any-order autoregressive models</em></h3><br>
      In this section, we introduce <strong>d2-AnyOrder</strong>, our trajectory likelihood estimator that can achieve unbiased trajectory 
      likelihood evaluation for masked DLMs with only one model pass. Since this estimator is inspired by the concept
      of any-order autoregressive models (AO-ARMs), we first take a detour to introduce AO-ARMs.<br>

      <br>The concept of AO-ARM is proposed in <a href="https://arxiv.org/abs/2110.02037"> Hoogeboom et al., 2021</a>,
      where they demonstrate that masked discrete diffusion models' training objective is equivalent to an any-order 
      autoregressive variant in the following form:

      $$
      \mathcal{L}_{\textnormal{AO-ARM}}=\mathbb{E}_{\sigma \sim U(S_D)}\Big[\sum_{l=1}^L \log p_\theta(\mathbf{x}_0^{\sigma(l)} \mid \mathbf{x}_0^{\sigma(&lt l)})\Big],
      $$

      where \(\sigma\) is a permutation of intergers \(1,\ldots,L\), and \(S_D\) is the set of all possible \(\sigma\).

      <h3 class="subtitle" style="font-size:1.6rem;"><em>d2-AnyOrder</em></h3><br>
      Inspired by the concept of AO-ARM, we propose that the trajectory likelihood \(\pi(\mathbf{x}_{0:T}^{1:L})\) can be rewritten as
      \(\prod_{l=1}^L\pi(\mathbf{x}_0^{\sigma(l)}\mid \mathbf{x}_0^{\sigma(\lt l)})\). Here, instead of a randomly sampled permutation, 
      \(\sigma\) records the decoding order of the sample tokens. In other words, \(\mathbf{x}_0^{\sigma(\lt l)}\) denotes the tokens
      decoded before \(\mathbf{x}_0^l\).<br> 
      
      <br>Based on this alternative decomposition, we propose our efficient and 
      effective trajectory likelihood estimator, d2-AnyOrder. Concretely, given a clean sample token sequence 
      \(\mathbf{x}_0^{1:L}\), we construct a 
      \(2L\)-length sequence \(\mathbf{x}_0^{1:L} \oplus \mathbf{m}^{L+1:2L}\), where \(\oplus\) denotes concatenation along the sequence 
      dimension and \(\mathbf{m}^{L+1:2L}\) are masked tokens. We assign the positional encoding as \(pos_l = l \; \textnormal{mod} \; L\). We then define the attention mask so that 
      a clean token \(\mathbf{x}_0^{\sigma(l)}\) attends to \(\mathbf{x}_0^{\sigma(\leq l)}\), and a mask token \(\mathbf{m}^{L+\sigma(l)}\) attends to 
      \(\mathbf{x}_0^{\sigma(\lt l)} \cup \mathbf{m}^{L+\sigma(l)}\). We then use the output logits of position \(L+l\) as a proxy of the logits of token
      \(\mathbf{x}_0^l\) at its decoding time step.

      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/d2anyorder.png" alt="Caduceus" style="width:500px;height:400px;">
        <br>
        Illustration of d2-AnyOrder, our one-shot, unbiased trajectory likelihood estimator. We depict attention with query tokens (one layer
        up) attending to keys / values (one layer below) via an undirected connected line. The output at each position is 
        depicted with a directed arrow. “pos” refers to positional encoding index. We use a
        three token example where the decoding order is ”for→d2→RL”.
      </h4>

      Denoting the resulting likelihood estimate as \(\pi^{\textnormal{AO}}(\mathbf{x}_0^l \mid \mathbf{x}_0^{1:L} \oplus \mathbf{m}^{L+1:2L})\), we 
      train the policy network with the following GRPO objective:

      $$
      \mathbb{E}_{\mathbf{x}_{0:T}^{1:L}\sim\pi_{\textnormal{old}}}\Big[\frac1L\sum_{l=1}^L\textnormal{min}\Big(\rho_{n,l}^{\textnormal{AO}}A^l, \textnormal{clip}(\rho_{n,l}^{\textnormal{AO}}, 1-\epsilon, 1+\epsilon)A^l\Big)+\beta D_{\textnormal{KL}}\Big(\pi_\theta(\mathbf{x}_{0:T}^{1:L} \mid \mathbf{q}) \Vert \pi_{\textnormal{ref}}(\mathbf{x}_{0:T}^{1:L}\mid \mathbf{q})\Big)\Big],
      $$

      where \(\rho_{n,l}^{\textnormal{AO}}=\frac{\pi_\theta^{\textnormal{AO}}(\mathbf{x}_0^l \mid \mathbf{x}_0^{1:L}\oplus \mathbf{m}^{L+1:2L}, \mathbf{q})}{\pi_{\textnormal{old}}^{\textnormal{AO}}(\mathbf{x}_0^l \mid \mathbf{x}_0^{1:L}\oplus\mathbf{m}^{L+1:2L}, \mathbf{q})}\). For simplicity, we will also call the RL algorithm induced by d2-AnyOrder <strong>d2-AnyOrder</strong> in the remainder of this blog post.
      
      <h3 class="subtitle" style="font-size:1.6rem;"><em>When does the any-order estimator work?</em></h3><br>
      Despite its simplicity, d2-AnyOrder does not yield unbiased trajectory likelihood evaluation for all masked DLMs naturally. In fact, d2-AnyOrder's unbiasedness is contingent on an assumption that
      \(\pi^{\textnormal{AO}}(\mathbf{x}_0^l \mid \mathbf{x}_0^{1:L} \oplus \mathbf{m}^{L+1:2L})\) should equal the probability \(\pi(\mathbf{x}_0^{\sigma(l)} \mid \mathbf{x}_0^{\sigma(\lt l)})\)
      of that token during sampling. Indeed, this property holds by construction when we sample from a masked DLM using a sampling algorithm called <strong>any-order decoding</strong>.

      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/anyorderdecoding_code.png" alt="Caduceus" style="width:800px;height:400px;">
        <br>
        Pseudocode of any-order decoding.
      </h4>

      In any-order decoding, at each time step, we input a partially masked token sequence \(\mathbf{x}^{1:L}\) (\(\mathbf{x}^l\) could either be a clean token, i.e., \(\mathbf{x}_0^l\) or 
      a masked token, i.e., \(\mathbf{m}^l\)) into the model
      and compute the logits at each masked position. Then \(k\) token positions are selected for unmasking based on certain heuristics, after 
      which unmasked tokens at selected positions are sampled and added to the token sequence. Notably, we set the attention mask of the transformer 
      parameterizing the DLM to satisfy the following two properties:

      <br><strong>Independent masks.</strong> Mask tokens do not attend to each other: they attend only to unmasked tokens and themselves.
      <br><strong>Order causality.</strong> Unmasked tokens attend only to tokens decoded at earlier time steps and to themselves.<br>

      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/anyorderdecoding.png" alt="Caduceus" style="width:800px;height:400px;">
        <br>
        Illustration of the any-order decoding algorithm for masked DLMs. This example follows the setting of the preivous figure, where three tokens are decoded in the order of "for→d2→RL". 
        At each time step, newly added attention relations in any-order decoding are highlighted with red line markers.
      </h4>

      <h3 class="subtitle" style="font-size:1.6rem;"><em>When does the any-order estimator not work?</em></h3><br>
      Any-order decoding can be applied to any masked DLM, which always yields samples whose likelihood can afterwards be computed in a single forward pass.
      Unfortunately, any-order decoding does not always produce high-quality samples. If the model was not trained with independent masks and order causality, 
      it may not produce good samples when these properties are introduced at inference time. We have empirically discovered that popular DLMs, such as LLaDA, 
      falls within this range of models (see evidence in our paper).
    </div>
  </div>
</section>
<!-- End Sampler-->


<!-- ELBO -->
<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <h2 class="title">d2-StepMerge: a practical trajectory likelihood approximator</h2>
    <div class="content is-medium">
      As noted above, not all DLMs support any-order decoding and thus may not support d2-AnyOrder by default. For these models, we propose our second estimator, d2-StepMerge, which, unlike d2-AnyOrder, only approximates the DLM's trajectory likelihood.<br> 

      <br>Since in this case the DLM model does not support d2-AnyOrder, we switch back to the standard trajectory likelihood 
      decomposition for masked DLMs, i.e., \(\pi(\mathbf{x}_{0:T}^{1:L})=\prod_{t=0}^{T-1}\prod_{l=1}^L\mathbf{1}_{t,l}\cdot\pi(\mathbf{x}_t^l \mid \mathbf{x}_t^{1:L})\).
      Computing this trajectory likelihood naively takes \(T\) model passes, rendering it computationally prohibitive. Consequently, we propose to cut the sample trajectory 
      of \(T\) tokens evenly into \(N\) contiguous time segments. For each time segment, we use the output of one model pass as a proxy for token likelihoods within this segment.
      Formally, the trajecotry likelihood is approxmiated as: 

      $$
        \pi(\mathbf{x}_{0:T}^{1:L}) \approx \prod_{n=0}^{N-1} \prod_{l=1}^L \mathbf{1}_{n,l} \cdot \pi(\mathbf{x}^l_{\frac{nT}{N}} \mid \mathbf{x}^{1:L}_{\frac{(n+1)T}{N}}),
      $$

      where \(\mathbf{1}_{n,l}=\mathbf{1}_{\mathbf{x}^l_{\frac{(n+1)T}{N}}=\mathbf{m}, \mathbf{x}^l_{\frac{nT}{N}}\neq\mathbf{m}}\) is the indicator of tokens decoded in the \(n_{\textnormal{th}}\) time segment.
      Based on this trjectory likelihood estimator, we train the policy network with the following GRPO objective:

      $$
      -\mathbb{E}_{\mathbf{x}_{0:T}^{1:L}\sim \pi_{\textnormal{old}}}\Big[\sum_{n=0}^{N-1}\frac1L\sum_{l=1}^L\mathbf{1}_{n,l}\textnormal{min}(\rho_n^lA^l, \textnormal{clip}(\rho_n^l, 1-\epsilon,1+\epsilon)A^l)+\beta D_{\textnormal{KL}}\Big(\pi_\theta(\mathbf{x}_{0:T}^{1:L}\mid \mathbf{q}) \Vert \pi_{\textnormal{ref}}(\mathbf{x}_{0:T}^{1:L}\mid \mathbf{q})\Big)\Big].
      $$

      Similarly to d2-AnyOrder, we will also call the RL algorithm induced by d2-StepMerge <strong>d2-StepMerge</strong> in the remainder of this blog post. 
    
      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/d2stepmerge.png" alt="Caduceus" style="width:1000px;height:300px;">
        <br>
        Illustration of the d2-StepMerge. In d2-StepMerge, we cut the trajectory evenly into \(N\) time segments and evaluate the likelihood for each segment together. Newly decoded tokens on which we compute the likelihood at the corresponding model forward pass are highlighted.
      </h4>

    </div>
  </div>
</section>
<!-- End ELBO-->


<!-- Experiments -->
<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <h2 class="title">Experimental results</h2>
    <div class="content is-medium">
      <h3 class="subtitle" style="font-size:1.6rem;"><em>d2-AnyOrder</em></h3><br>
        We first evaluate d2-AnyOrder on Eso-LM (<a href="https://arxiv.org/abs/2506.01928">
      Sahoo et al., 2025</a>), a 190M parameter masked DLM trained from scratch on Open-Web-Text. The training algorithm
      of Eso-LM ensures that it supports any-order decoding. As shown in the following table, on a task of toxiciy steering
      (<a href="https://arxiv.org/abs/2501.06848">Singhal et al., 2025</a>) in which the model is supposed to steer up toxicity 
      score, d2-AnyOrder significantly dominates the correpsonding baseline under the same compute budget.<br>

      <br>
      <table class="results-table">
  <caption>
    Table: Toxicity Score vs. FLOPs. 
    Our d2-AnyOrder approach significantly dominates the DDPO baseline 
    in toxicity steering for a given compute budget.
  </caption>

  <thead>
    <tr>
      <th rowspan="2">Method</th>
      <th colspan="6">FLOPs × 10<sup>17</sup></th>
    </tr>
    <tr>
      <th>0.00</th>
      <th>0.25</th>
      <th>0.50</th>
      <th>0.75</th>
      <th>1.00</th>
      <th>1.25</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>DDPO toxicity</td>
      <td>-9.2</td>
      <td>-9.2</td>
      <td>-9.1</td>
      <td>-8.9</td>
      <td>-8.9</td>
      <td>-8.6</td>
    </tr>

    <tr>
      <td>d2 toxicity (ours)</td>
      <td>-9.2</td>
      <td><b>-8.5</b></td>
      <td><b>-7.3</b></td>
      <td><b>-5.5</b></td>
      <td><b>-2.7</b></td>
      <td><b>-0.7</b></td>
    </tr>
  </tbody>
</table>
    <br>

    We then evaluate d2-AnyOrder on an any-order causal LLaDA model that we finetune from LLaDA-8B-Instruct (see our paper for detailed finetuning recipe).
    As shown in the following figure, d2-AnyOrder strictly dominates diffu-GRPO, where d2-AnyOrder consistently pushes up the GSM8K test set accuracy while
    diffu-GRPO barely does so.

    <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/d2anyorder_result.png" alt="Caduceus" style="width:600px;height:400px;">
        <br>
        Performance-compute dynamics of d2-AnyOrder and diffu-GRPO on the any-order causal LLaDA checkpoint that we finetuned.
      </h4>

    </div>
  </div>
</section>
<!-- Experiments -->


<!-- Conclusion -->
<section class="section" id="Conclusion">
  <div class="container is-max-desktop">
    <h2 class="title">Conclusion</h2>
    <div class="content is-medium">
      In this work, we have presented d2, a principled RL framework for diffusion language models grounded in a formal policy gradient derivation.
      We introduce d2-AnyOrder for unbiased trajectory likelihood estimates with a single model pass, contingent on the model's supporting a simple sampling algorithm called any-order decoding. For models that do not naturally support any-order decoding, we propose a second estimator, d2-StepMerge, which, unlike d2-AnyOrder, only approximates the trajectory likelihood. Empirically, d2 achieves superior performance compared to widely used RL baselines on DLMs that support any-order decoding, and demonstrates state-of-the-art performance on four math and logical reasoning benchmarks, without relying on supervised chain-of-thought finetuning.
    </div>
  </div>
</section>
<!-- Conclusion -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{wang2025d2,
          title={d2: Improved techniques for training reasoning diffusion language models},
          author={Wang, Guanghan and Schiff, Yair and Turok, Gilad and Kuleshov, Volodymyr},
          journal={arXiv preprint arXiv:2509.21474},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
